\chapter{Background and Related Works}
\label{cha:embed}

\section{Neural Probabilistic Language Model} 
This section will introduce a neural probabilistic language model from \citep{bengio2003neural}. Such model use a very important tool---Word Embedding. So what is the word embedding? General speaking, for any word $w$ in the dictionary $D$, one can specify a fixed length of real-valued vector $v(w)\in \mathbb{R}^m$, $v(w)$ called the word embedding of $w$, and $m$ is the length of word embedding. A further understanding about the word embeddings will be explained in the next section. 

Since it is a neural probabilistic language model, it is obvious to use an neural network. Figure \ref{fig:neural4} shows the structure of the neural network, it include 4 layers: \textbf{Input} layer, \textbf{Projection} layer, \textbf{Hidden} layer and the \text{Output} layer. $W$ and $U$ are respectively the weight matrix between projection layer and hidden layer and the weight matrix between hidden layer and output layer, \textbf{p} and \textbf{q} are the offset vectors of respectively the hidden layer and the output layer.
 
\begin{figure}[!ht]
  \centering
	\fbox{\includegraphics[width=0.7\textwidth]{neural4} }
	\caption{An example of 4 layers neural network}
	\label{fig:neural4}
\end{figure}
 
When talking about the above neural network, generally we consider it as a three-layer structure as following Figure \ref{fig:neural3}. But this thesis still use the structure of Figure \ref{fig:neural4}. On the one hand it is easy to describe, on the other hand it is more convenient to do comparison with the network structure in word2vec.

\begin{figure}[!ht]
  \centering
	\fbox{\includegraphics[width=0.5\textwidth]{neural3} }
	\caption{An example of 3 layers neural network}
	\label{fig:neural3}
\end{figure}

\citep{bengio2003neural} also considers the connection of some neurons from projection layer and some neurons from hidden layer as Figure \ref{fig:bengio}. Thus, there is one more weight matrix. In the numerical experiments, the author found that the introduction of the weight matrix projection layer and output layer can not improve the model effect, but it can reduce the number of training iterations.\\

\begin{figure}[!ht]
  \centering
	\fbox{\includegraphics[width=0.7\textwidth]{bengio} }
	\caption{The neural network structure from \citep{bengio2003neural}}
	\label{fig:bengio}
\end{figure}

For any word $w$ in corpus $C$ , assuming $Context(w)$ takes its front $n-1$ words (similar to the n-gram), this binary pair $(Context (w), w)$ is a a training sample. So how is the sample $(Context (w), w)$ involved in computing through the neural network? Note that once word corpus $C$ and vector length $m$ is given, the scale of projection layer and the scale of output layer are determined. The former is $(n-1)m$, the latter is $N=|D|$, that is, the size of vocabulary. The size of the hidden layer $n_h$ is the adjustable parameter which can be specified by the user.

Why is the size of the projected layer $(n-1)m$? In fact, the input layer includes $n-1$ words from $Context(w)$, and the vector $\mathbf{x_w}$ from projected layer is built: concatenate $n-1$ input word vectors to be a long vector whose length is $(n-1)m$. With the vector $\mathbf{x_w}$ , the next calculation is clear
\begin{equation}\label{eq:neural4}
\left\{
\begin{aligned}
\mathbf{z_w} & =  \tanh(W x_w+\mathbf{p}),\\
\mathbf{y_w} & =  U z_w + \mathbf{q}\\
\end{aligned}
\right.
\end{equation}
where $\tanh$ is the Hyperbolic Tangent Function, used as the Active Function in the hidden layer. In the above formula, $\tanh$ acting on the vector represents acting each component of the vector. How about the number of first few words of a given sentence is less than $n-1$? Usually, we can artificially add some filler vectors, and they will also be involved in the training process.\\

From the above two steps, we get $\mathbf{y_w}=(y_{w,1},y_{w,2},\ldots,y_{w,N})^T$, which is just the vector with the length of $N$ and its components can not represent probabilities. If you want to use $\mathbf{y_w}$'s component $y_ {w,i}$ to represent that the probability of the next word is the $i$-th word when the context is $Context(w)$. Also you need to do a softmax normalization. After normalization, $p(w|Context(w))$ can be expressed as
\begin{equation}\label{eq:softmax}
p(w|Context(w))=\frac{e^{y_{w,i_w}}}{\sum^N_{i=1}e^{y_{w,i}}},
\end{equation}
where $i_w$ represents the index of $w$ in the dictionary $D$.

Formula \ref{eq:softmax} gives the function representation of the probability $p(w|Context(w))$, that is, it gives the function mentioned in the last section $F(w,Context(w),\theta)$. So what is the $\theta$? In conclusion, there are two parts \\
\begin{itemize}
\item Word vectors: $\mathbf{v}(w)\in \mathbb{R}^m, w\in D$ and the filter vectors
\item Neural network parameters: $W\in \mathbb{R}^{n_h*(n-1)m}, \mathbf{p}\in \mathbb{R}^{n_h}; U \in \mathbb{R}^{N*n_h}, \mathbf{q}\in \mathbb{R}^N$
\end{itemize}
These parameters can be obtained by the training algorithm. On thing needs to be mentioned that, in common machine learning algorithms, the input is already known, however in the above neural probability language model shown as Figure \ref{fig:neural4}, the input $\mathbf{v}(w)$ is not known and also needs to be training.

The next, let's loot at the computation of the above model. In the above neural network, the scales of the projection layer, the hidden layer and the output layer are respectively $(n-1)m$, $n_h$, $N$, let's look at the parameters involved:
\begin{enumerate}
\item $n$ is the number of words contained in the context of a word, usually no more than 5
\item $m$ is the length of word vector, usually the magnitude of $10^1\sim10^2$
\item $n_h$ is specified by the customer, usually not too big, like the magnitude of $10^2$
\item $N$ is the size of corpus vocabulary , related with the corpus, usually the magnitude of $10^4\sim10^5$  
\end{enumerate}
Recombination with \ref{eq:neural4} and \ref{eq:softmax}, it is not difficult to find that, the computing of entire model is mainly about the matrix-vector operations between the hidden layer and the output layer and the softmax normalization in the output layer. Therefore, many of subsequent related works are about optimization for this part, including the the work of word2vec. 

Comparison with n-gram model, neural probabilistic language model mainly has the following two advantages:

\begin{enumerate}
\item Similarity between words can be be reflected in the word vectors.

\tab If in an (English) corpus, $S_1$ = \lq\lq A dog is running in the room\rq\rq\ appears 10000 times, and $S_2$ = \lq\lq A cat is running in the room\rq\rq\ only appears once. According to the n-gram model, $p(S_1)$ will certainly be much greater than $p(S_2)$. Note that the only difference between $S_1$ and $S_2$ is the \lq\lq dog\rq\rq\ and \lq\lq cat\rq\rq\ , but this two words play the same role either semantically or grammatically, so $p(S_1)$ and $p(S_2)$ should be very close.

\tab However, the probabilities $p(S_1)$ and $p(S_2)$ calculated by the neural network are approximately equal. The reason is that: 
\begin{enumerate}
\item In the neural network probabilistic language model, there is an assumption that the \lq\lq similar \rq\rq\ words should have similar vectors
\item The probability function on the word vectors is smooth, that is there is only very small influence for the probability when word vector change a little. 
\end{enumerate}
As a result, for the following sentences
\begin{labeling}{alligator}
\item [\tab A dog is running in the room] 
\item [\tab A cat is running in the room]
\item [\tab The cat is running in a room] 
\item [\tab A dog is walking in a bedroom] 
\item [\tab The dog was walking in the room] 
\item [\tab \tab ...] 
\end{labeling}
anyone appears in the corpus, the probabilities of other sentences will increase accordingly.
\item Models based on the word vector have smoothing already (from \ref{eq:softmax}, we can know $p(w|Context(w))\in(0,1)$ can not be $0$), no longer need to carry the additional processing like n-gram model.
\end{enumerate}

Finally, let's look back and think about what kind of role the word vector plays in the neural probability model. When training, it is just the auxiliary parameter used to construct the objective function; after the training, it seems just a by-product of the language model. However, this by-product can not be underestimated, the next section will be further elaborate its usefulness.
%--------------------------------------------------------------------------------------------------------------------------------%
\section{Understanding of the Word Embedding}
In NLP tasks, we will use machine learning algorithms to deal with natural language, but the machine can not directly understand human language, so the first thing is to transform the language to the mathematical form. How can we do such thing? Word vector provides a solution.\\

One of the easiest word vector is one-hot representation, which is to use a long vector to represent a word, the vector's length is  $N$, the size of dictionary $D$. It only has one component which is 1, and the other components are all 0s. The position of 1 corresponds to the index of the word in dictionary. But this word vector representation has some disadvantages, such as troubled by the huge dimensionality, especially when it is applied to deep learning scenes; another thing, it can not describe the similarity between words very well. Another word vector is Distributed Representation, it was firstly proposed by \cite{williams1986learning}, which can overcome the above drawbacks from one-hot representation. The basic idea is to train the particular language to map each word into a short vector of fixed length (here \lq\lq short\rq\rq\ is respected to \lq\lq long\rq\rq\ in one-hot representation). All of these vectors constitute a vector space, and each can be regarded as a a point in the vector space. After introducing the \lq\lq distance\rq\rq in this space , it is possible to judge the similarity between words (morphology and syntax) according to the distance. Actually, word2vec uses this Distributed Representation for word vector.\\

Why is it called Distributed Representation? For one-hot representation, there is only one non-zero vector component, which is very concentrated. For Distributed Representation, vectors have a lot of non-zero components, relatively dispersed. It distributes the information of the word into each component, which is very similar as distributed parallel.\\

Suppose that there are $a$ different points distributed on the two-dimensional plane, giving a point from them, the task is to find another point closest to this point in the plane. How can we do it? Firstly, establish a Cartesian coordinate system. Based on this coordinate system, each point on which uniquely corresponds to a coordinate $(x, y)$; and then introduce the Euclidean distance; finally calculate the distance between this point and other $a-1$ points, from which the point with the minimum distance is the one we are looking for. In the above example, the role of the coordinates $(x, y)$ is equivalent to the word vector. It is used to mathematically quantify a point on a plane. After the coordinate system is set up, it is very easy to get the coordinate of a point. However, for NLP tasks, to get the word vector is more complex, and the word vector is not unique, which depends on the quality of the training data, training algorithm and other factors.\\

A good word vector is valuable, for example, Ronan Collobert's team makes use of the word vector from software package SENNA (\citep{collobert2011natural}) to do POS, CHK, NER and other tasks, and achieves good results. Google's Tomas Mikolov team has developed an automatic generation technology for dictionary and glossary, which is able to convert one language into another language. The relation collection between words in each language, that is \lq\lq language space\rq\rq\ , can be characterized as a set of vectors in the mathematical sense.  As long as the mapping and translation of a vector space to another vector space are realized, language translation can be realized. This technique has very good performance for translation between English and Spanish, with the accuracy rate up to $90\%$. 

\section{C$\&$W's Model}

C$\&$W's original main purpose is not to generate a good word vectors, or even do not want to train the language model, but to use this word vectors to complete several tasks from natural language processing, such as speech tagging, named entity recognition, phrase recognition, semantic role labeling, and so on (\citep{collobert2008unified} and \citep{collobert2011natural}). Due to the different purpose, their training method is also different and special. They do not use language model's idea like optimizing the probability $P( w_t| w_1, w_2, ... , w_{t - 1})$, but directly use the score $f( w_{t - n + 1}, ... , w_{t - 1}, w_t)$ to determine if the sentence is reasonable and normal; low score illustrates the sentence is not reasonable; if you put a few words randomly together, it would be certainly a negative score. The score is just about high or low, not business with probabilities. 

With the above assumption, C$\&$W used the pair-wise method to train the word vectors. Specifically, it is to minimize the following objective function.

$$\sum_{x\in X}\sum_{w\in D}max\{0,1-f(x),f(x^{(w)})\}$$

$X$ is the set of all consecutive $n$-length phrases, D is the entire dictionary. The first summation enumerates all $n$-length phrases from the training set, and each of them positive sample. The second summation for dictionary is to build negative samples. $x(w)$ means the phrase $x$ replacing the middle word to the word $w$. In most cases, replacing the middle of the word in a normal phrase, the new phrase is certainly not the normal phrase, which is a good method to build negative sample (in most cases they are negative samples, only in rare cases the normal phrases are considered as negative samples but they would not affect the final result). \\

The structure of $f$ is almost save as the network structure from \citep{bengio2003neural}. The same thing is connecting $n$ word vectors together to get a long vector and passing through one layer (a matrix multiplication) to get the hidden layer. The difference is that C$\&$W's output layer has only one node representing the score, rather than Bengio's $|V|$ nodes. Doing so greatly reduced the computational complexity. Of course, C$\&$W does not want to make a real language model, but just use some idea from the language model to assist them to complete other tasks in NLP. 

Specifically, they give two different neural network structures window approach and sentence approach, shown as Figure \ref{fig:cw1} and Figure \ref{fig:cw2} respectively. Window approach is a feedforward neural network including a linear layer, HardTanh layer. Its input is the the vector concatenated by all all word vectors within the current word window including itself. Window approach is able to deal with most of natural language processing tasks, but has very poor performance on SRL taks. There, they proposed sentence approach to solve such problem. It is convolutional neural network structure. Apart from the linear layer and HardTanh layer, it has another convolutional layer and Max layer. 

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
 
	\includegraphics[width=0.9\textwidth]{cw1} 
	\caption{window approach(\cite{collobert2011natural})}
	\label{fig:cw1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  
	\includegraphics[width=0.85\textwidth]{cw2}
	\caption{sentence approach (\citep{collobert2011natural})}
	\label{fig:cw2}
\end{minipage}
\end{figure}

In the experiment the size of window $n$ is 11 and the size of dictionary $|V|$ is 130000. They spent totally seven weeks to train word vectors from the Wikipedia English corpus and Reuters corpus.

\section{Word2Vec}
This section will introduce two important model in word2vec: CBOW model (Continuous Bag-of-Words Model) and Skip-gram model (Continuous Skip-gram Model). 

From the figure, two models both include three layers: \textbf{Input Layer}, \textbf{Projection Layer}, \textbf{Output Layer}. The former is to predict the current word $w_t$ giving its context $w_{t-2}$,$w_{t-1}$,$w_{t+1}$,$w_{t+2}$

With the foregoing preparation, this section describes word2vec officially used in two important models --CBOW model (Coutinuous Bag-of-Words Model) and Skip-gram model (Continuous Skip-gram Model). About two models, \cite{mikolov2013distributed} shows the schematic diagram shown in Figures \ref{fig:CBOW} and \ref{fig:Skip-Gram}.
Be seen by the two models contain three layers: \textbf{Input layer}, \textbf{projection layer}  and \textbf{output layer}. The former is known in the current word $w_t$ context $w_{t-2}$, $w_{t-1}$, $w_{t+1}$, $w_{t+2}$ premise predictive current word $w_t$ (see Figure 8); and the latter on the contrary, it is known in the current word $w_t$ premise predict its context $w_{t-2}$, $w_{t-1}$ , $w_{t+1}$, $w_{t+2}$ (see Figure 9).
For two CBOW and Skip-gram model, word2vec given two frameworks, which are based on Hierarchical Softmax and Negative Sampling to design. This section describes the Hierarchical Softmax CBOW and Skip-gram model.
In the previous section, we mentioned that the objective function neural network based language model is generally taken as follows log-likelihood function
$$\mathcal{L}=\sum_{w\in\mathcal{C}}\mathrm{log}\ p(w|Context(w)), $$
The key is the conditional probability function $p(w|Context(w))$ configuration, text [] in this model is given a construction method function (see (3.6) formula).
For the objective function Hierarchical Softmax CBOW word2vec model based on optimized also the form (4.1); and for the objective function based on Hierarchical Softmax of Skip-gram model, the optimization of the form
$$\mathcal{L}=\sum_{w\in\mathcal{C}}\mathrm{log}\ p(Context(w)|w), $$
Therefore, the discussion process, we should focus on the $p(w|Context(w))$ or $p(Context(w)|w)$ on the structure, realize that this is very important because it allows us to targeted, distractions, and will not fall into some of the tedious details were to go. Next, we will focus on the Skip-gram model with negative sampling and explain some mathematical details, because our model is based that.

\begin{center}
\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
 
	\includegraphics[width=0.9\textwidth]{CBOW} 
	\caption{CBOW model}
	\label{fig:CBOW}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  
	\includegraphics[width=0.85\textwidth]{Skip-Gram}
	\caption{Skip-Gram model}
	\label{fig:Skip-Gram}
\end{minipage}
\end{figure}
\end{center}

\subsection{Skip-gram model with Hierarchical Softmax}
This section describes word2vec another model -- Skip-gram model, since the derivation and CBOW similar, and therefore will inherit the measure introduced mark.

Figure 12 shows the network structure of Skip-gram model, with network structure CBOW model, it also includes three layers: an input layer, a projection layer and output layer. The following sample $(w,Context(w))$, for example, three layers are described briefly.
\begin{enumerate}
\item \textbf{input layer}: the center of the current sample containing only the word $w$ word vector $\mathbf{v}(w)\in\mathbb{R}^m$.
\item \textbf{projection layer}: This projection is identical to $\mathbf{v}(w)$ projection to $\mathbf{v}(w)$. Therefore, this projection layer is actually superfluous reason here mainly to facilitate retention projection layer and network structure CBOW models do comparison.
\item \textbf{Output layer}: and CBOW model, the output layer is also a lesson Huffman tree.
\end{enumerate}
\subsubsection {Gradient Calculation} 
For Skip-gram model, it is known that the current word $w$, need to predict its context $Context(w)$ of the words, the objective function should therefore form (4.2), and the key is the conditional probability function $p(Context(w)|w)$ configuration, in the Skip-gram model which is defined as
$$p(Context(w)|w)=\prod_{u\in Context(w)}^{p(u|w},$$
In the above formula $p(u|w)$ in accordance with section describes the Hierarchical Softmax thought, similar to (4.3) written as
$$p(u|w)=\prod_{j = 2}^{l^u}p(d^u_j|\text{v}(w),\theta^u_{j-1}), $$
among them
\begin{equation}
p(d^u_j|\mathbf{v}(w),\theta^u_{j-1})=[\theta(\mathbf{v}(w)^{\mathrm{T}}\theta^u_{j-1})]^{1-d^u_j}\cdot[1-\theta(\mathbf{v}(w)^{\mathrm{T}}\theta^u_{j-1})]^{1-d^u_j}
\end{equation}
The (4.6) followed by generations back, you can get the log-likelihood function (4.2) of the specific expression
\begin{equation}
\mathcal{L}=\sum_{w\in\mathcal{C}}\mathrm{log}\prod_{u\in Context(w)}\prod_{j=2}^{l^u}\{[\theta(\mathbf{v}(w)^{\mathrm{T}}\theta^u_{j-1})]^{1-d^u_j}\cdot[1-\theta(\mathbf{v}(w)^{\mathrm{T}}\theta^u_{j-1})]^{\ d^u_j}\} \\
\ \ \ $$ $$=\sum_{w\in\mathcal{C}}\sum_{u\in Context(w)}\sum_{j=2}^{l^u}\{(1-d^u_j)\cdot\mathrm{log}[\theta(\mathbf{v}(w)^{\mathrm{T}}\theta^u_{j-1})]+d^u_j\cdot\mathrm{log}[1-\theta(\mathbf {v}(w)^{\mathrm{T}}\theta^u_{j-1})]\}.
\end{equation}
Similarly, as in the following gradients of convenience, under the triple summation symbol braces contents of abbreviated as $\mathcal{L}(w,u,j)$, ie
$$\mathcal{L}(w,u,j)=(1-d^u_j)\cdot\mathrm{log}[\theta(\mathbf{v}(w)^{\mathrm{T}}\theta^u_{j-1})]+d^u_j\cdot\mathrm{log}[1-\theta(\mathbf{v}(w)^{\mathrm{T}}\theta^u_{j-1}]. $$
So far, it has been deduced logarithmic likelihood function of expressions like (4.7), which is the objective function Skip-gram model. Then also use stochastic gradient ascent method to optimize the key is to give two types of gradients.
First consider $\mathcal{L}(w,u,j)$ on $\theta^u_{j-1}$ gradient calculation (with the corresponding portion of the model is derived CBOW completely analogous).
$$\partial\frac{\mathcal{L}(w, u, j)}{\partial\theta^u_{j-1}}=\frac{\partial}{\partial\theta^u_{j-1}}\{(1-d^u_j)\cdot\mathrm{log}[\theta(\mathbf{v}(w)^{\mathrm{T}}\theta^u_{j-1})]+d^u_j\cdot\mathrm{log}[\theta(\mathbf{v}(w)^{\mathrm{T}}\theta^u_{j-1})]\}$$

\subsection{Skip-gram with Negative Sampling}

Negative Sampling (NEG) is proposed by Tomas Mikolov et al.[word2vec]  , which is the simplified version of NCE(Noise Contrastive Estimation), the purpose is to improve the training and the quality of word vectors. Comparison with Hierarchical Softmax, NEG do not use the Huffman tree. Instead, it use Random Negative Sampling, which can improve the performance much.

The details of NCE is a little complex, the essence is to use a known probability density function to estimate an unknown probability density function. In short, assume there is an unknown probability density function $Y$ and a known probability density function $X$, if we get the relationship between $X$ and $Y$, we can obtain $X$ as well.The detail of method reference to [NCE]. 

The objective function is:
\begin{equation}
G=\prod_{w\in\mathcal{C}}\prod_{u\in Context(w)}g(u),
\end{equation}
Here, we want to maximize $\prod_{u\in Context(w)}g(u)$ giving $(w, Context(w)))$,  and $g(u)$ is defined as
$$g(u)=\prod_{z\in{u}\cup NEG(u)}p(z|w),$$
where $NEG(u)$ represents the negative samples generated by $u$, the conditional probability
$$p(z|w)=\left\{
\begin{aligned}
\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z), && L^u(z)=1; \\
1-\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z), && L^u(z)=0; \\
\end{aligned}
\right.
$$
where $$L^u(z) = \left\{
\begin{aligned}
1, && u = z;\\
0, && u \neq z,\\
\end{aligned}
\right.
$$
It can also be written as one expression
\begin{equation}
p(z|w)=[\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)]^{L^u(z)}\cdot[1-\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)]^{1-L^u(z)}
\end{equation}
And then we use the log of $G$, so the final objective function is 
\begin{align*}
L & =\mathrm{log}\ G=\mathrm{log} \prod_{w\in\mathcal{C}}\prod_{u\in Context(w)} g(u)=\sum_{w\in\mathcal{C}}\sum_{u\in Context(w)} \mathrm{log}\ g(u) \\
& = \sum_{w\in\mathcal{C}}\sum_{u\in Context(w)} \mathrm{log} \prod_{z\in\{u\}\cup NEG(u)} p(z|w) \\
& = \sum_{w\in\mathcal{C}}\sum_{u\in Context(w)}\sum_{z\in\{u\}\cup NEG(u)} \mathrm{log}\ p(z|w) \\
& = \sum_{w\in\mathcal{C}}\sum_{u\in Context(w)}\sum_{z\in\{u\}\cup NEG(u)} \mathrm{log}\ \{[\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)]^{L^u(z)}\cdot[1-\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)]^{1-L^u(z)}\} \\
& = \sum_{w\in\mathcal{C}}\sum_{u\in Context(w)}\sum_{z\in\{u\}\cup NEG(u)}\{L^u(z)\cdot \mathrm{log}[\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)]+[1-L^u(z)]\cdot\mathrm{log}[1-\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)]\}.
\end{align*}
In order to calculate the gradient more conveniently, we use $L(w,u,z)$ to represent the contents of curly braces as
$$\mathcal{L}(w,u,z)=L^u(z)\cdot \mathrm{log}[\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)]+[1-L^u(z)]\cdot\mathrm{log}[1-\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)]$$
And next, let's use \textbf{Stochastic gradient ascent method} to optimize it. The point is to calculate two kinds of gradient. Let's consider the gradient $\theta^z$ firstly.
\begin{align*}
& \ \ \ \ \frac{\partial\mathcal{L}(w,u,z)}{\partial\theta^z} \\
& =  \frac{\partial}{\partial\theta^z} \{ L^u(z)\cdot \mathrm{log}[\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)]+[1-L^u(z)]\cdot\mathrm{log}[1-\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)] \} \\
& =  L^u(z)[1-\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)]\mathbf{v}(w) - [1-L^u(z)]\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)\mathbf{v}(w) \\
& = \{L^u(z)[1-\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)]-[1-L^u(z)]\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)\}\mathbf{v}(w) \\
& = [L^u(z)-\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)] \mathbf{v}(w).
\end{align*}
Thus, the updating formula of $\theta^z$ can be written as
$$\theta^z:=\theta^z+\eta[L^u(z)-\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)]\mathbf{v}(w).$$
The next, let's consider the gradient of $\mathbf{v}(w)$. Using the symmetry of \textbf{v}(w) and $\theta^z$, we have
$$\frac{\partial\mathcal{L}(w,u,z)}{\partial\mathbf{v}(w)} = [L^u(z)-\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)]\theta^z,$$
Thus, the updating formula of $\mathbf{v}(u)$ can be written as 
$$\mathbf{v}(w):=\mathbf{v}(w)+\eta\sum_{z\in\{u\}\cup NEG\{u\}}\frac{\partial\mathcal{L}(w,u,z)}{\partial\mathbf{v}(w)}$$
$$=\mathbf{v}(w)+\eta\sum_{z\in\{u\}\cup NEG\{u\}}[L^u(z)-\sigma(\mathbf{v}(w)^{\mathrm{T}}\theta^z)]\theta^z.$$

\section{Huang's Model}


Eric H. Huang's work (\cite{huang2012improving}) is based on the model from \cite{collobert2008unified}. The goal of his working is about trying to make the word vectors with richer semantic information than other models. He had two major innovations to accomplish this goal : The first innovation is using global information from the whole text to assist local information, the second innovation is using the multiple word vectors to represent polysemy. \\

Huang thinks C$\&$W's work uses only "local context". In the process of training vectors, C$\&$W used only 10 words as the context for each word, counting the center word itself, there are totally 11 words' information. 
These local information can not fully exploit the semantic information of the center word. Huang used C$\&$W's neural network directly to compute a score as the "local score". 
And then Huang proposed a "global information", which is somewhat similar to the traditional bag of words model. Bag of words is about accumulating One-hot Representation from all the words of the article together to form a vector (like all the words thrown in a bag), which is used to represent the article. Huang's global information used the average weighted vectors from all words in the article (weight is word's idf), which is considered the semantic of the article. 
He connected such semantic vector of the article (global information) 
with the current word's vector (local information) to form a new vector with double size as an input, and then used the C$\&$W's network to calculate the score. Figure [huang] shows such structure.
With the "local score" from original C$\&$W approach and "Global score" from improving method based on the C$\&$W approach, Huang directly add two scores as the final score. The final score would be optimized by the pair-wise target function from C$\&$W. Huang found his model can capture better semantic information. \\

\begin{figure}[!ht]
  \centering
	\fbox{\includegraphics[width=0.9\textwidth]{huang} }
	\caption{The network structure from \citep{huang2012improving}}
	\label{fig:huang}
\end{figure}


The second contribution of this paper is to represent polysemy using multiple word vectors. \citep{bengio2003neural} also mentioned that this would be an very important issue, but he was still looking for a solution, now Huang gives an idea. For each center word, he took 10 nearest context words and calculated the weighted average of these 10 word vectors (idf weights) as the context vector. Huang used all context vectors to do k-means clustering, and relabel  each word based on the clustering results (different classes of the same words would be considered as different words to process), and finally re-trained the word vectors. The following gives some examples from his model's results.\\

\begin{center}

 \begin{tabular}{|l|l|}
  \hline
  Center Word &Nearest Neighbors \\
  \hline  
  bank$\_$1 & corporation, insurance, company\\
  \hline
  bank$\_$2 & shore, coast, direction\\
  \hline
  star$\_$1 & movie, film, radio\\
  \hline
  star$\_$2 & galaxy, planet, moon\\
  \hline
  cell$\_$1 & telephone, smart, phone\\
  \hline
  cell$\_$2 & pathology, molecular, physiology\\
  \hline
  left$\_$1 & close, leave, live\\
  \hline
  left$\_$2 & top, round, right\\
  \hline
 \end{tabular}
\end{center}
\section{EM-Algorithm based method}


There is a very famous method based on the EM-algorithm from \cite{tian2014probabilistic}. This method is the extension of normal skip-gram model. They still use each center word to predict several context words. The difference is that each center word can have several senses with different probabilities. The probability can represent if a sense is used frequent in the corpus. For example, considering $bank_1$ and $bank_2$, $bank_1$ represents the side of the river with the smaller probability and $bank_2$ means the institute about money with the higher probability. We can say in the corpus, in most sentences of the corpus the word "bank" means the institute about money and in other fewer cases it means the side of the river. \\

\paragraph{Objective Function}\ \\

Considering $w_I$ as the input word and $w_O$ as the output word, $(w_I,w_O)$ is a data sample. The input word $w_I$ have $N_{w_I}$ prototypes, and it appears in its $h_{w_I}$-th prototype, i.e., $h_{w_I}\in \{1,..,N_{w_I}\}$ [] The prediction $P(w_O|w_I)$ is like the following formula

$$p(w_O|w_I)=\sum^{N_{w_I}}_{i=1}P(w_O|h_{w_I}=i,w_I)P(h_{w_I}=i|w_I)=\sum^{N_{w_I}}_{i=1}\frac{exp(U^{\mathrm{T}}_{w_O}V_{w_I,i})}{\sum_{w\in W exp(U^\mathrm{T}_w V_{w_I,i})}}P(h_{w_I}=i|w_I)$$

where $V_{w_I,i}\in R^d$ refers to the d-dimensional "input" embedding vector of $w_I$'s $i$-th prototype and $U_{w_O}\in R^d$ represents the "output" embedding vectors of $w_O$. Specifically, they use the Hierarchical Softmax Tree function to approximate the probability calculation. 

\paragraph{Algorithm Description}\ \\

Particularly for the input word $w$, they put all samples ($w$ as the input word) together like $\{(w, w_1), (w, w_2), (w, w_3) ... (w, w_n)\}$ as a group. Each group is based on the input word. So the whole training set can be separated as several groups. For the group mentioned above, one can assume the input word $w$ has $m$ vectors ($m$ senses), each with the probability $p_j (1 \leq j \leq m)$. And each output word $w_i (1 \leq i\leq n)$ has only one vector. \\

 In the training process, for each iteration, they fetch only part of the whole training set and then split it into several groups based on the input word. In each E-step, for the group mentioned above, they used soft label $y_{i,j}$ to represent the probability of input word in sample $(w,w_i)$ assigned to the $j$-th sense. The calculating of $y_{i,j}$ is based on the value of sense probability and sense vectors. After calculating each $y_{i,j}$ in each data group, in the M-step, they use $y_{i,j}$ to update sense probabilities and sense vectors from input word, and the word vectors from output word. The following are some results from this model.


\begin{tabular}{|l|l|l|}
  \hline
  word & Prior Probability & Most Similar Words \\
  \hline  
  apple$\_$1 & 0.82 & strawberry, cherry, blueberry\\
  \hline
  apple$\_$2 & 0.17 & iphone, macintosh, microsoft\\
  \hline
  bank$\_$1 & 0.15 & river, canal, waterway\\
  \hline
  bank$\_$2 & 0.6 & citibank , jpmorgan, bancorp\\
  \hline
  bank$\_$3 & 0.25 & stock, exchange, banking\\
  \hline
  cell$\_$1 & 0.09 & phones cellphones, mobile\\
  \hline
  cell$\_$2 & 0.81 & protein, tissues, lysis\\
  \hline
  cell$\_$3 & 0.01 &locked , escape , handcuffed\\
  \hline
 \end{tabular}