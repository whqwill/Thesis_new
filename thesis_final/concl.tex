\chapter{Conclusion}
\label{cha:concl}

To conclude this paper, In chapter 2, we introduced several word embedding methods including the details of gradient calculation in skip-gram model with negative sampling. In Chapter 3, we introduced three different sense embedding models. Based on these models, we described our mathematical model to generate sense embedding vectors in Chapter 4 and introduced the implementation using Spark in Chapter 5. After that we compared different experiments and analyzed different hyper-parameters with running time, loss of validation set and score of word similarity task. Originally our model assume that for each word both input embedding and output embedding have multiple senses. But the the experiment result told us, output would be better have only one sense. We displayed the nearest words of different senses from the same word. The result showed that our model can really derive expressive sense embeddings, which achieved our goal.

The spark framework is very convenient to use. In the processing of training our word embedding vectors, we gain many turning experience of the techniques. And the experiments showed that our implementation is really efficient, that is also our goal.

However, the evaluation on similarity tasks seems not very satisfied comparing with other models. Maybe in the future, we can do more related working to improve our model. We can try bigger size of embedding vector. Of course, we should in the mean time deal with the memory problem introduced by bigger vector size. On anther hand, we can also do more pre works such as remove the stop words, which may also improve our results. And the max number of senses in our model is only three, we will more number of senses and try to extend our model so that it can decide the number of senses for each word as NP-MSSG (\citep{NeelakantanShankarEtAl2015}). Further more, we think we can do more experiments for the same hyper-parameters in the future to make our results more reliable.
