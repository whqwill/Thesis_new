\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@input{title.aux}
\providecommand\@newglossary[4]{}
\@newglossary{symbols}{glg}{sym}{sbl}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{thesis.ist}
\@glsorder{word}
\@input{acknowledgements.aux}
\@input{abstract.aux}
\citation{BengioDucharmeEtAl2003}
\citation{HuangSocherEtAl2012}
\citation{TianDaiEtAl2014}
\citation{SaltonWongEtAl1975}
\citation{CollobertWeston2008}
\citation{MikolovSutskeverEtAl2013}
\citation{SocherPerelyginEtAl2013}
\citation{ZhouXu2015}
\citation{CollobertWestonEtAl2011}
\citation{Harris1954}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:introduction}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Word Embedding}{1}{section.1.1}}
\citation{DeerwesterDumaisEtAl1990}
\citation{CollobertWeston2008}
\citation{MikolovSutskeverEtAl2013}
\citation{PenningtonSocherEtAl2014}
\citation{Fellbaum1998}
\newlabel{fig:neighbouring_words}{{1.1}{2}{Word Embedding}{section.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Neigboring words defining the specific sense of "bank".}}{2}{figure.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Sense Embedding}{2}{section.1.2}}
\citation{BleiNgEtAl2003}
\citation{HuangSocherEtAl2012}
\citation{ChenLiuEtAl2014}
\citation{TianDaiEtAl2014}
\citation{ZahariaChowdhuryEtAl2010}
\citation{MikolovSutskeverEtAl2013}
\citation{NeelakantanShankarEtAl2015}
\citation{MikolovSutskeverEtAl2013}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Goal}{4}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Outline}{4}{section.1.4}}
\citation{WilliamsHinton1986}
\citation{DeerwesterDumaisEtAl1990}
\citation{BleiNgEtAl2003}
\citation{BengioDucharmeEtAl2003}
\citation{CollobertWestonEtAl2011}
\citation{MikolovSutskeverEtAl2013}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:background}{{2}{5}{Background}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Word Embedding}{5}{section.2.1}}
\citation{CollobertWestonEtAl2011}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Graph of the $tanh$ function}}{6}{figure.2.1}}
\newlabel{fig:tanh}{{2.1}{6}{Graph of the $tanh$ function}{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Neural Probabilistic Language Model}{6}{section.2.2}}
\@writefile{toc}{\contentsline {paragraph}{Neural Network}{6}{section*.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces An example of neural network with three layers}}{7}{figure.2.2}}
\newlabel{fig:neural3}{{2.2}{7}{An example of neural network with three layers}{figure.2.2}{}}
\newlabel{eq:neural}{{2.2}{7}{Neural Network}{equation.2.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Statistical Language Model}{7}{section*.9}}
\citation{BengioDucharmeEtAl2003}
\citation{BengioDucharmeEtAl2003}
\citation{BengioDucharmeEtAl2003}
\newlabel{eq:languagemodel}{{2.4}{8}{Statistical Language Model}{equation.2.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Neural Probabilistic Language Model}{8}{section*.10}}
\newlabel{eq:bengio}{{2.5}{8}{Neural Probabilistic Language Model}{equation.2.2.5}{}}
\citation{BengioDucharmeEtAl2003}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The neural network structure from \citep  {BengioDucharmeEtAl2003}}}{9}{figure.2.3}}
\newlabel{fig:bengio}{{2.3}{9}{The neural network structure from \citep {BengioDucharmeEtAl2003}}{figure.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Softmax Function}{9}{section*.11}}
\citation{CollobertWeston2008}
\citation{CollobertWeston2008}
\citation{CollobertWestonEtAl2011}
\citation{CollobertWeston2008}
\citation{BengioDucharmeEtAl2003}
\newlabel{eq:softmax}{{2.7}{10}{Softmax Function}{equation.2.2.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}The model of Collobert and Weston}{10}{section.2.3}}
\newlabel{eq:skipgram}{{2.8}{11}{The model of Collobert and Weston}{equation.2.3.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Word2Vec}{11}{section.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces word2vec}}{12}{figure.2.4}}
\newlabel{fig:word2vec}{{2.4}{12}{word2vec}{figure.2.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Skip-gram model with negative sampling}{12}{section*.12}}
\citation{HuangSocherEtAl2012}
\citation{CollobertWeston2008}
\citation{CollobertWeston2008}
\citation{HuangSocherEtAl2012}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related Works}{15}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:relatedworks}{{3}{15}{Related Works}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Huang's Model}{15}{section.3.1}}
\citation{TianDaiEtAl2014}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The network structure from \citep  {HuangSocherEtAl2012}}}{16}{figure.3.1}}
\newlabel{fig:huang}{{3.1}{16}{The network structure from \citep {HuangSocherEtAl2012}}{figure.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Senses computed with Huang's network and their nearest neighbors.}}{16}{table.3.1}}
\newlabel{tab:huang}{{3.1}{16}{Senses computed with Huang's network and their nearest neighbors}{table.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}EM-Algorithm based method}{16}{section.3.2}}
\citation{TianDaiEtAl2014}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Word senses computed by \citeauthor  {TianDaiEtAl2014}}}{17}{table.3.2}}
\newlabel{tab:tian}{{3.2}{17}{Word senses computed by \citeauthor {TianDaiEtAl2014}}{table.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Objective Function}{17}{section*.13}}
\@writefile{toc}{\contentsline {paragraph}{Algorithm Description}{17}{section*.14}}
\citation{neelakantan2015efficient}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}A Method to Determine the Number of Senses}{18}{section.3.3}}
\@writefile{toc}{\contentsline {paragraph}{MSSG}{18}{section*.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Architecture of MSSG model with window size $R_t=2$ and $K=3$ }}{19}{figure.3.2}}
\newlabel{fig:MSSG}{{3.2}{19}{Architecture of MSSG model with window size $R_t=2$ and $K=3$}{figure.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{NP-MSSG}{19}{section*.16}}
\citation{HuangSocherEtAl2012}
\citation{TianDaiEtAl2014}
\citation{MikolovSutskeverEtAl2013}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Solution}{21}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:solution}{{4}{21}{Solution}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Definition}{21}{section.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Objective Function}{22}{section.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Algorithm Description}{24}{section.4.3}}
\@writefile{toc}{\contentsline {paragraph}{Initialization}{25}{section*.17}}
\@writefile{toc}{\contentsline {paragraph}{Sense Probabilities}{25}{section*.18}}
\@writefile{toc}{\contentsline {subparagraph}{}{25}{section*.19}}
\@writefile{toc}{\contentsline {subparagraph}{}{26}{section*.20}}
\@writefile{toc}{\contentsline {paragraph}{}{26}{section*.21}}
\citation{ZahariaChowdhuryEtAl2010}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Implementation}{29}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:implementation}{{5}{29}{Implementation}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction of Spark}{29}{section.5.1}}
\citation{Shaoul2010}
\citation{HuangSocherEtAl2012}
\citation{TianDaiEtAl2014}
\citation{NeelakantanShankarEtAl2015}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Implementation}{30}{section.5.2}}
\@writefile{toc}{\contentsline {paragraph}{Gradient Checking}{30}{section*.22}}
\@writefile{toc}{\contentsline {paragraph}{Data preparing}{30}{section*.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Shows the accumulated frequency of word count in range [1,51]}}{32}{figure.5.1}}
\newlabel{fig:1to51}{{5.1}{32}{Shows the accumulated frequency of word count in range [1,51]}{figure.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Shows the accumulated frequency of word count in range [51,637]}}{32}{figure.5.2}}
\newlabel{fig:51to637}{{5.2}{32}{Shows the accumulated frequency of word count in range [51,637]}{figure.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Shows the accumulated frequency of word count in range [637,31140]}}{33}{figure.5.3}}
\newlabel{fig:637to31140}{{5.3}{33}{Shows the accumulated frequency of word count in range [637,31140]}{figure.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Shows the accumulated frequency of word count in range [637,919787]}}{33}{figure.5.4}}
\newlabel{fig:31140torest}{{5.4}{33}{Shows the accumulated frequency of word count in range [637,919787]}{figure.5.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Computing Environment}{34}{section*.24}}
\@writefile{toc}{\contentsline {paragraph}{Training set and validation set}{34}{section*.25}}
\@writefile{toc}{\contentsline {paragraph}{Learning Rate Reduction}{34}{section*.26}}
\@writefile{toc}{\contentsline {paragraph}{Iteration}{35}{section*.27}}
\@writefile{toc}{\contentsline {paragraph}{Assign Step}{35}{section*.28}}
\@writefile{toc}{\contentsline {paragraph}{Learn Step}{35}{section*.29}}
\@writefile{toc}{\contentsline {paragraph}{Normalization}{36}{section*.30}}
\citation{FinkelsteinGabrilovichEtAl2001}
\citation{HuangSocherEtAl2012}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Evaluation}{37}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:evaluation}{{6}{37}{Evaluation}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Results for different Hyper-Parameters}{38}{section.6.1}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Definition of Hyper-Parameters of the Experiments }}{39}{table.6.1}}
\newlabel{tab:notationhyper}{{6.1}{39}{Definition of Hyper-Parameters of the Experiments}{table.6.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Definition of Evaluation Scores }}{39}{table.6.2}}
\newlabel{tab:notationevalution}{{6.2}{39}{Definition of Evaluation Scores}{table.6.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces 13 Different Experiments in Step 1}}{40}{table.6.3}}
\newlabel{tab:experiment13}{{6.3}{40}{13 Different Experiments in Step 1}{table.6.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Different sizes of embedding vectors}{40}{section*.31}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces 16 Different Experiments in Step 2}}{41}{table.6.4}}
\newlabel{tab:experiment16}{{6.4}{41}{16 Different Experiments in Step 2}{table.6.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces Different Vector Size Comparison}}{41}{table.6.5}}
\newlabel{tab:group1}{{6.5}{41}{Different Vector Size Comparison}{table.6.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Different Min Count}{41}{section*.32}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Shows the effect of varying embedding dimensionality of our model on the Time}}{42}{figure.6.1}}
\newlabel{fig:vectime}{{6.1}{42}{Shows the effect of varying embedding dimensionality of our model on the Time}{figure.6.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces Different Min Count Comparison}}{42}{table.6.6}}
\newlabel{tab:group2}{{6.6}{42}{Different Min Count Comparison}{table.6.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Shows the effect of varying embedding dimensionality of our model on the loss of validation set}}{43}{figure.6.2}}
\newlabel{fig:vecloss}{{6.2}{43}{Shows the effect of varying embedding dimensionality of our model on the loss of validation set}{figure.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Shows the effect of varying embedding dimensionality of our model on the SCWS task}}{43}{figure.6.3}}
\newlabel{fig:vecSCWS}{{6.3}{43}{Shows the effect of varying embedding dimensionality of our model on the SCWS task}{figure.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Shows the effect of varying embedding dimensionality of our model on the WordSim-353 task}}{44}{figure.6.4}}
\newlabel{fig:vecword353}{{6.4}{44}{Shows the effect of varying embedding dimensionality of our model on the WordSim-353 task}{figure.6.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.7}{\ignorespaces Different Sense Count Comparison}}{45}{table.6.7}}
\newlabel{tab:group3}{{6.7}{45}{Different Sense Count Comparison}{table.6.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Different Sense Count Comparison}{45}{section*.33}}
\@writefile{toc}{\contentsline {paragraph}{Different Learning Rate and Gamma}{45}{section*.34}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Shows the number of words with different number of senses from three experiments}}{46}{figure.6.5}}
\newlabel{fig:sensecount}{{6.5}{46}{Shows the number of words with different number of senses from three experiments}{figure.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Shows the effect of varying beginning learning rate on the best loss of validation set}}{46}{figure.6.6}}
\newlabel{fig:lrloss}{{6.6}{46}{Shows the effect of varying beginning learning rate on the best loss of validation set}{figure.6.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.8}{\ignorespaces Different Learning Rate Comparison}}{47}{table.6.8}}
\newlabel{tab:group41}{{6.8}{47}{Different Learning Rate Comparison}{table.6.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.9}{\ignorespaces Different Gamma Comparison}}{47}{table.6.9}}
\newlabel{tab:group42}{{6.9}{47}{Different Gamma Comparison}{table.6.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Shows the effect of varying beginning learning rate on the total number of training iterations}}{47}{figure.6.7}}
\newlabel{fig:lriter}{{6.7}{47}{Shows the effect of varying beginning learning rate on the total number of training iterations}{figure.6.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Shows the effect of reduction factor of the learning rate on the best loss of validation set}}{48}{figure.6.8}}
\newlabel{fig:gmloss}{{6.8}{48}{Shows the effect of reduction factor of the learning rate on the best loss of validation set}{figure.6.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Shows the effect of reduction factor of the learning rate on the total number of training iterations}}{48}{figure.6.9}}
\newlabel{fig:gmiter}{{6.9}{48}{Shows the effect of reduction factor of the learning rate on the total number of training iterations}{figure.6.9}{}}
\citation{HuangSocherEtAl2012}
\citation{CollobertWeston2008}
\citation{MikolovSutskeverEtAl2013}
\citation{HuangSocherEtAl2012}
\citation{NeelakantanShankarEtAl2015}
\@writefile{toc}{\contentsline {paragraph}{Different Number of Output Senses}{49}{section*.35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Comparison to prior analyses}{49}{subsection.6.1.1}}
\citation{MikolovSutskeverEtAl2013}
\@writefile{lot}{\contentsline {table}{\numberline {6.10}{\ignorespaces Comparison of the different number of output senses}}{50}{table.6.10}}
\newlabel{tab:group5}{{6.10}{50}{Comparison of the different number of output senses}{table.6.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.11}{\ignorespaces Nearest words comparison}}{50}{table.6.11}}
\newlabel{tab:nearestcompare}{{6.11}{50}{Nearest words comparison}{table.6.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.12}{\ignorespaces Experimental results in the SCWS task. The numbers are Spearmans correlation $\rho $ $\times $ 100}}{51}{table.6.12}}
\newlabel{tab:SCWS}{{6.12}{51}{Experimental results in the SCWS task. The numbers are Spearmans correlation $\rho $ $\times $ 100}{table.6.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.13}{\ignorespaces Results on the WordSim-353 dataset}}{51}{table.6.13}}
\newlabel{tab:word353}{{6.13}{51}{Results on the WordSim-353 dataset}{table.6.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Case Analysis}{51}{section.6.2}}
\@writefile{lot}{\contentsline {table}{\numberline {6.14}{\ignorespaces Sense Similarity Matrix of $apple$}}{51}{table.6.14}}
\newlabel{tab:sensematrixapple}{{6.14}{51}{Sense Similarity Matrix of $apple$}{table.6.14}{}}
\citation{MaatenHinton2008}
\@writefile{lot}{\contentsline {table}{\numberline {6.15}{\ignorespaces Nearest Words of $apple$}}{52}{table.6.15}}
\newlabel{tab:nearestapple}{{6.15}{52}{Nearest Words of $apple$}{table.6.15}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.16}{\ignorespaces Sentence Examples of $apple$}}{52}{table.6.16}}
\newlabel{tab:sentenceapple}{{6.16}{52}{Sentence Examples of $apple$}{table.6.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Nearest words from $apple$}}{53}{figure.6.10}}
\newlabel{fig:apple}{{6.10}{53}{Nearest words from $apple$}{figure.6.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.17}{\ignorespaces Nearest words from $fox$ , \ $net$ , \ $rock$ , \ $run$ and \ $plant$}}{54}{table.6.17}}
\newlabel{tab:nearestwordsother}{{6.17}{54}{Nearest words from $fox$ , \ $net$ , \ $rock$ , \ $run$ and \ $plant$}{table.6.17}{}}
\@writefile{toc}{\contentsline {paragraph}{}{54}{section*.36}}
\@writefile{toc}{\contentsline {paragraph}{}{54}{section*.37}}
\@writefile{lot}{\contentsline {table}{\numberline {6.18}{\ignorespaces Sentence Examples of $fox$ , \ $net$ , \ $rock$ , \ $run$ and \ $plant$ }}{55}{table.6.18}}
\newlabel{tab:sentenceother}{{6.18}{55}{Sentence Examples of $fox$ , \ $net$ , \ $rock$ , \ $run$ and \ $plant$}{table.6.18}{}}
\@writefile{toc}{\contentsline {paragraph}{}{55}{section*.38}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Nearest words from $apple$,\ $fox$,\ $net$,\ $rock$,\ $run$ and $plant$}}{56}{figure.6.11}}
\newlabel{fig:keywords20}{{6.11}{56}{Nearest words from $apple$,\ $fox$,\ $net$,\ $rock$,\ $run$ and $plant$}{figure.6.11}{}}
\citation{NeelakantanShankarEtAl2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{57}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:concl}{{7}{57}{Conclusion}{chapter.7}{}}
\bibstyle{apalike}
\bibdata{thesis}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix}{59}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:appendix}{{A}{59}{Appendix}{appendix.A}{}}
\bibcite{BengioDucharmeEtAl2003}{{1}{2003}{{Bengio et~al.}}{{}}}
\bibcite{BleiNgEtAl2003}{{2}{2003}{{Blei et~al.}}{{}}}
\bibcite{ChenLiuEtAl2014}{{3}{2014}{{Chen et~al.}}{{}}}
\bibcite{CollobertWestonEtAl2011}{{4}{2011}{{Collobert et~al.}}{{}}}
\bibcite{CollobertWeston2008}{{5}{2008}{{Collobert and Weston}}{{}}}
\bibcite{DeerwesterDumaisEtAl1990}{{6}{1990}{{Deerwester et~al.}}{{}}}
\bibcite{Fellbaum1998}{{7}{1998}{{Fellbaum}}{{}}}
\bibcite{FinkelsteinGabrilovichEtAl2001}{{8}{2001}{{Finkelstein et~al.}}{{}}}
\bibcite{Harris1954}{{9}{1954}{{Harris}}{{}}}
\bibcite{HuangSocherEtAl2012}{{10}{2012}{{Huang et~al.}}{{}}}
\bibcite{MaatenHinton2008}{{11}{2008}{{Maaten and Hinton}}{{}}}
\bibcite{MikolovSutskeverEtAl2013}{{12}{2013}{{Mikolov et~al.}}{{}}}
\bibcite{NeelakantanShankarEtAl2015}{{13}{2015}{{Neelakantan et~al.}}{{}}}
\bibcite{PenningtonSocherEtAl2014}{{14}{2014}{{Pennington et~al.}}{{}}}
\bibcite{SaltonWongEtAl1975}{{15}{1975}{{Salton et~al.}}{{}}}
\bibcite{Shaoul2010}{{16}{2010}{{Shaoul}}{{}}}
\bibcite{SocherPerelyginEtAl2013}{{17}{2013}{{Socher et~al.}}{{}}}
\bibcite{TianDaiEtAl2014}{{18}{2014}{{Tian et~al.}}{{}}}
\bibcite{WilliamsHinton1986}{{19}{1986}{{Williams and Hinton}}{{}}}
\bibcite{ZahariaChowdhuryEtAl2010}{{20}{2010}{{Zaharia et~al.}}{{}}}
\bibcite{ZhouXu2015}{{21}{2015}{{Zhou and Xu}}{{}}}
\ulp@afterend
